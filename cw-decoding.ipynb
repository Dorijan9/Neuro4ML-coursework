{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFnP5PWrctES"
      },
      "source": [
        "# Neuro4ML Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, each question that needs a response will be marked by a ⚠️.\n",
        "\n",
        "In this coursework, you will use a data set of spike trains recorded from monkey motor cortex while it was doing a task involving moving a pointer on a screen. The aim of this coursework is to decode the recorded velocity of the pointer from the neural data using a network of leaky integrate-and-fire neurons that take the recorded spikes as input and give sequences of velocities as outputs. You will train these networks using surrogate gradient descent. If you haven't already looked at it, a great starting point is Friedemann Zenke's [SPyTorch tutorial notebook 1](https://github.com/fzenke/spytorch/blob/main/notebooks/SpyTorchTutorial1.ipynb) (and the rest are worth looking at too).\n",
        "\n",
        "In this coursework, we are following the general approach of the article [\"Machine learning for neural decoding\" (Glaser et al. 2020)](https://doi.org/10.1523/ENEURO.0506-19.2020), but using a spiking neural network decoder instead of the statistical and artificial neural network models used in that paper. You can also have a look at the [GitHub repository for the paper](https://github.com/KordingLab/Neural_Decoding). In case you're interested, the data were originally recorded for the paper [\"Population coding of conditional probability distributions in dorsal premotor cortex\" (Glaser et al. 2018)](https://doi.org/10.1038/s41467-018-04062-6), but you do not need to read this paper to understand this coursework.\n",
        "\n",
        "The general setup is illustrated in this figure:\n",
        "\n",
        "![Cartoon of decoder setup](cartoon.png)\n",
        "\n",
        "You are given an array of ``num_neurons`` spike trains in a variable ``spike_trains``. This variable is a Python list of numpy arrays, each numpy array has a different length and is the recorded times (in seconds) that the corresponding neuron fired a spike. You also have two additional arrays ``vel`` and ``vel_times`` where ``vel`` has shape ``(num_time_points, 2)`` and ``vel_times`` has has shape ``(num_time_points)``. The second axis of ``vel`` has length 2 corresponding to the x and y-components of the recorded velocity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugUY4mlbdQ2Z"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaymEeXAc6Dp"
      },
      "source": [
        "This section has some basics to get you started.\n",
        "\n",
        "Let's start by importing some libraries you can make use of. You can solve all the task only using the imports below, but you are welcome to add your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ags-Wad--GP"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from scipy import io\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as tnnf\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "ms = 1e-3 # use this constant so you can write e.g. 1*ms for a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GHpT0oVc9pw"
      },
      "source": [
        "You already have a copy of the raw data, but for your information, here is where the original can be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request, zipfile, os\n",
        "filename = 's1_data_raw.mat'\n",
        "if not os.path.exists(filename):\n",
        "    urllib.request.urlretrieve('https://www.dropbox.com/sh/n4924ipcfjqc0t6/AACPWjxDKPEzQiXKUUFriFkJa?dl=1', 'data.zip')\n",
        "    with zipfile.ZipFile('data.zip') as z:\n",
        "        z.extract(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTPju6FQdb8i"
      },
      "source": [
        "## Task 1: Load and plot the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycf87WWXdGQx"
      },
      "source": [
        "The code below first loads the raw data, which is stored as a Matlab file, and then extracts the three arrays ``spike_times``, ``vel`` and ``vel_times``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FwkbNQnFC_AN"
      },
      "outputs": [],
      "source": [
        "# Load the raw data\n",
        "data = io.loadmat('s1_data_raw.mat') # a matlab file!\n",
        "spike_times = [st[:, 0] for st in data['spike_times'].ravel()] # a list of arrays of spike times in seconds, one for each neuron, spike times in seconds\n",
        "vel = data['vels'] # velocity data shape (num_time_points, 2) for (x, y) coordinates\n",
        "vel_times = data['vel_times'].squeeze() # times the velocities were recorded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWWyklhNdJZi"
      },
      "source": [
        "### Task 1A: Preprocess and compute basic statistics\n",
        "\n",
        "In this task, you will preprocess the data, extract some basic statistics from it.\n",
        "\n",
        "1. ⚠️ Whiten the recorded velocities (i.e. transform them so that their mean is 0 and standard deviation is 1).\n",
        "2. ⚠️ Compute and print out the number of neurons and number of spikes recorded.\n",
        "3. ⚠️ Compute and print out the duration of the experiment in seconds and/or minutes.\n",
        "4. ⚠️ Compute and print out the sampling rate at which spikes were recorded (or find the information in the corresponding paper).\n",
        "5. ⚠️ Compute and print out the sampling rate at which velocities were recorded (or find the information in the corresponding paper).\n",
        "\n",
        "Note that the spikes and velocities were recorded with different equipment and so they have different sampling rates. Think about how you can estimate these sampling rates from the recorded data (or look it up in the paper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Velocity Whitening:\n",
            "   Original mean (x, y): (-0.0000, 0.0000)\n",
            "   Original std (x, y): (1.0000, 1.0000)\n",
            "   Whitened mean (x, y): (-0.0000, 0.0000)\n",
            "   Whitened std (x, y): (1.0000, 1.0000)\n",
            "\n",
            "   Neural Data Statistics:\n",
            "   Number of neurons: 52\n",
            "   Total number of spikes: 1,480,032\n",
            "   Average spikes per neuron: 28462.15\n",
            "\n",
            "3. Experiment Duration:\n",
            "Duration (from velocities): 3066.99 seconds\n",
            "Duration (from velocities): 51.12 minutes\n",
            "First spike time: 0.01 seconds\n",
            "Last spike time: 3071.33 seconds\n",
            "Velocity time range: 1.17 to 3068.16 seconds\n",
            "\n",
            "4. Spike Sampling Rate:\n",
            "   Mean firing rate across all neurons: 9.28 spikes/second/neuron\n",
            "   Minimum time difference between spikes: 0.0333 ms\n",
            "   Estimated temporal resolution: 30000 Hz\n",
            "   Note: Spike recording equipment typically samples at ~30 kHz (30000 Hz)\n",
            "   but this refers to the recording resolution, not the spike rate\n",
            "\n",
            "5. Velocity Sampling Rate:\n",
            "   Number of velocity time points: 306,700\n",
            "   Velocity sampling rate: 100.00 Hz\n",
            "   Velocity sampling period: 10.00 ms\n",
            "   (Velocities are typically recorded at 60 Hz for this type of data)\n",
            "\n",
            "============================================================\n",
            "SUMMARY:\n",
            "  Neurons: 52\n",
            "  Total spikes: 1,480,032\n",
            "  Duration: 3066.99 s (51.12 min)\n",
            "  Velocity sampling: 100.00 Hz\n",
            "  Mean firing rate: 9.28 sp/s/neuron\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Whiten the recorded velocities (mean=0, std=1)\n",
        "vel_mean = np.mean(vel, axis=0) #mean is 0\n",
        "vel_std = np.std(vel, axis=0) #std is 1\n",
        "vel_whitened = (vel - vel_mean) / vel_std #whitening formula\n",
        "\n",
        "print(\"   Velocity Whitening:\")\n",
        "print(f\"   Original mean (x, y): ({vel_mean[0]:.4f}, {vel_mean[1]:.4f})\")\n",
        "print(f\"   Original std (x, y): ({vel_std[0]:.4f}, {vel_std[1]:.4f})\")\n",
        "print(f\"   Whitened mean (x, y): ({np.mean(vel_whitened[:, 0]):.4f}, {np.mean(vel_whitened[:, 1]):.4f})\")\n",
        "print(f\"   Whitened std (x, y): ({np.std(vel_whitened[:, 0]):.4f}, {np.std(vel_whitened[:, 1]):.4f})\")\n",
        "print()\n",
        "\n",
        "\n",
        "vel = vel_whitened # Update vel to use whitened version for future tasks\n",
        "\n",
        "num_neurons = len(spike_times) #number of neurons\n",
        "total_spikes = sum(len(st) for st in spike_times) #total number of spikes\n",
        "\n",
        "print(\"   Neural Data Statistics:\")\n",
        "print(f\"   Number of neurons: {num_neurons}\")\n",
        "print(f\"   Total number of spikes: {total_spikes:,}\")\n",
        "print(f\"   Average spikes per neuron: {total_spikes / num_neurons:.2f}\")\n",
        "print()\n",
        "\n",
        "\n",
        "max_spike_time = 0 #max spike time\n",
        "min_spike_time = float('inf') #min spike time\n",
        "\n",
        "for neuron_spikes in spike_times: #for each neuron\n",
        "    if len(neuron_spikes) > 0: #if the neuron has spikes\n",
        "        neuron_max = max(neuron_spikes) #max spike time\n",
        "        neuron_min = min(neuron_spikes) #min spike time\n",
        "        if neuron_max > max_spike_time: #update max spike time\n",
        "            max_spike_time = neuron_max\n",
        "        if neuron_min < min_spike_time: #update min spike time\n",
        "            min_spike_time = neuron_min\n",
        "duration_from_vel = vel_times[-1] - vel_times[0] #duration from velocity data\n",
        "duration_minutes = duration_from_vel / 60 #duration in minutes\n",
        "\n",
        "print(\"3. Experiment Duration:\")\n",
        "print(f\"Duration (from velocities): {duration_from_vel:.2f} seconds\")\n",
        "print(f\"Duration (from velocities): {duration_minutes:.2f} minutes\")\n",
        "print(f\"First spike time: {min_spike_time:.2f} seconds\")\n",
        "print(f\"Last spike time: {max_spike_time:.2f} seconds\")\n",
        "print(f\"Velocity time range: {vel_times[0]:.2f} to {vel_times[-1]:.2f} seconds\")\n",
        "print()\n",
        "\n",
        "# 4. Compute and print spike sampling rate\n",
        "# Spikes are event-based, not regularly sampled\n",
        "# We can estimate the effective sampling rate from the data\n",
        "# or look it up in the paper (typically ~30 kHz for recording equipment)\n",
        "# However, the actual spike rate is much lower (events per second per neuron)\n",
        "\n",
        "# Compute mean firing rate across all neurons\n",
        "mean_firing_rate = total_spikes / (num_neurons * duration_from_vel)\n",
        "\n",
        "# For spike data, the \"sampling rate\" refers to the recording equipment resolution\n",
        "# This is typically very high (30 kHz = 30000 Hz) but we can't determine this from the data alone\n",
        "# We can estimate the temporal resolution from the minimum time difference between spikes\n",
        "all_spike_times = np.concatenate(spike_times)\n",
        "all_spike_times_sorted = np.sort(all_spike_times)\n",
        "time_diffs = np.diff(all_spike_times_sorted)\n",
        "min_time_diff = np.min(time_diffs[time_diffs > 0])  # Minimum non-zero time difference\n",
        "estimated_spike_resolution = 1.0 / min_time_diff if min_time_diff > 0 else None\n",
        "\n",
        "print(\"4. Spike Sampling Rate:\")\n",
        "print(f\"   Mean firing rate across all neurons: {mean_firing_rate:.2f} spikes/second/neuron\")\n",
        "print(f\"   Minimum time difference between spikes: {min_time_diff*1000:.4f} ms\")\n",
        "if estimated_spike_resolution:\n",
        "    print(f\"   Estimated temporal resolution: {estimated_spike_resolution:.0f} Hz\")\n",
        "print(f\"   Note: Spike recording equipment typically samples at ~30 kHz (30000 Hz)\")\n",
        "print(f\"   but this refers to the recording resolution, not the spike rate\")\n",
        "print()\n",
        "\n",
        "# 5. Compute and print velocity sampling rate\n",
        "num_vel_points = len(vel_times)\n",
        "velocity_sampling_rate = num_vel_points / duration_from_vel\n",
        "velocity_sampling_period = 1.0 / velocity_sampling_rate\n",
        "\n",
        "print(\"5. Velocity Sampling Rate:\")\n",
        "print(f\"   Number of velocity time points: {num_vel_points:,}\")\n",
        "print(f\"   Velocity sampling rate: {velocity_sampling_rate:.2f} Hz\")\n",
        "print(f\"   Velocity sampling period: {velocity_sampling_period*1000:.2f} ms\")\n",
        "print(f\"   (Velocities are typically recorded at 60 Hz for this type of data)\")\n",
        "print()\n",
        "\n",
        "# Summary\n",
        "print(\"=\" * 60)\n",
        "print(\"SUMMARY:\")\n",
        "print(f\"  Neurons: {num_neurons}\")\n",
        "print(f\"  Total spikes: {total_spikes:,}\")\n",
        "print(f\"  Duration: {duration_from_vel:.2f} s ({duration_minutes:.2f} min)\")\n",
        "print(f\"  Velocity sampling: {velocity_sampling_rate:.2f} Hz\")\n",
        "print(f\"  Mean firing rate: {mean_firing_rate:.2f} sp/s/neuron\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1B: Plot the data\n",
        "\n",
        "In this task, you will plot the data to get a feeling for what it is like (an important step in any modelling).\n",
        "\n",
        "1. ⚠️ Plot the spike times as a raster plot (black dots at x-coordinates the time of the spike, and y-coordinates the index of the neuron). Plot this both for the whole data set and for the period from 1000 to 1010 seconds.\n",
        "2. ⚠️ Plot the x- and y-coordinates of the velocities. Plot this both for the whole data set and for the same period as above for the spikes.\n",
        "3. ⚠️ Compute the mean firing rate (number of spikes per second) for each neuron and display as a bar chart.\n",
        "4. ⚠️ Plot the velocities as a curve in (x, y) space, emphasising the part of the velocity curve for the period above.\n",
        "\n",
        "You can use the template below to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# # Plot all spikes\n",
        "# ax = plt.subplot(231)\n",
        "# # ...\n",
        "# plt.ylabel('Neuron index')\n",
        "# # Plot all x- and y- components of the velocities\n",
        "# plt.subplot(234, sharex=ax)\n",
        "# # ...\n",
        "# plt.xlabel('Time (s)')\n",
        "# plt.ylabel('velocity')\n",
        "# plt.legend(loc='best')\n",
        "# # Plot spikes at times t=1000 to t=1010\n",
        "# ax = plt.subplot(232)\n",
        "# # ...\n",
        "# # Plot velocities at times t=1000 to t=1010\n",
        "# plt.subplot(235, sharex=ax)\n",
        "# plt.xlabel('Time (s)')\n",
        "\n",
        "# # Compute firing rates for each neuron and plot as a histogram\n",
        "# plt.subplot(233)\n",
        "# firing_rates = ...\n",
        "# plt.barh(range(len(spike_times)), firing_rates, height=1)\n",
        "# plt.xlabel('Firing rate (sp/s)')\n",
        "# plt.ylabel('Neuron index')\n",
        "\n",
        "# # Plot all velocities as points in (x, y)-plane as a continuous curve\n",
        "# # Emphasise the region from t=1000 to t=1010 with a different colour\n",
        "# plt.subplot(236)\n",
        "# # ...\n",
        "# plt.xlabel('X velocity')\n",
        "# plt.ylabel('Y velocity')\n",
        "\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKYHCPj0dmih"
      },
      "source": [
        "## Task 2: Divide data into test/train and batches\n",
        "\n",
        "In this section, you will divide the data up into non-overlapping test, train and (optionally) validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2A: Why not just use continuous time ranges?\n",
        "\n",
        "In the Glaser et al. paper linked to above, they simply divide the range so that the first 70% of the data is used for training, the next 15% for validation and the last 15% for testing. This is a standard thing to do.\n",
        "\n",
        "1. ⚠️ Compute the speeds from the velocity components, plot them and compute some statistics to show why this is a bad idea for this dataset. To make the plot clear, you might want to take averages over some window rather than plotting every point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2B: use equally distributed segments for training, test and validation\n",
        "\n",
        "1. ⚠️ Divide the data up into segments and use some fraction of these segments for training, test and validation, ensuring these segments do not overlap but that they are equally distributed throughout the data.\n",
        "2. ⚠️ Write a generator function (see below) ``batched_data`` that iterates over your data in randomly ordered segments of a given length, returning it in batches. The function should have arguments that determine the fraction of the data to use, the simulation time step that will be used, the length (in seconds) of each batch, and the batch size (you may add additional arguments if you wish). The function should return a pair of arrays ``(x, y)``. The array ``x`` has shape ``(batch_size, num_neurons, num_time_points)`` containing the spike times as an array where a zero indicates no spike and 1 indicates a spike. Here ``num_time_points`` is the number of time points in the batch measured at the sampling rate of the simulation time step, not the number of time points in the data as a whole, nor at the spike or velocity sampling rate. The array ``y`` has shape ``(batch_size, 2, num_time_points)`` containing the velocities at the same time points as the spikes. You will need to use some sort of interpolation to get the velocities at these times.\n",
        "3. ⚠️ Plot a sample of spike times and velocities for a random batch of length 1 second and ``batch_size=4``.\n",
        "\n",
        "#### Note on generator functions\n",
        "\n",
        "Generator functions are an advanced feature of Python that makes it easy to iterate over complicated datasets. The general syntax is just a standard function that uses the keyword ``yield`` instead of ``return`` to return data, which allows it to return data multiple times. You can iterate over the values returned by a generator function instead of just calling it. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gimme_some_multiples(n):\n",
        "    yield n*1\n",
        "    yield n*2\n",
        "    yield n*3\n",
        "\n",
        "for x in gimme_some_multiples(3):\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And another:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gimme_some_more_multiples(n, how_many):\n",
        "    for i in range(how_many):\n",
        "        yield n*(i+1)\n",
        "\n",
        "for x in gimme_some_more_multiples(5, 4):\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Template\n",
        "\n",
        "You can use the following template but you may want to define some additional helper functions to simplify your code and that you can re-use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Template\n",
        "\n",
        "# # Set up training / validation / testing ranges\n",
        "# # ...\n",
        "\n",
        "# # Generator function that yields batches\n",
        "# def batched_data(range_to_use, dt=1e-3, length=1, batch_size=64):\n",
        "#   pass # ...\n",
        "#   for batch_idx in range(num_batches):\n",
        "#     x = torch.zeros((batch_size, num_neurons, num_time_points))\n",
        "#     y = torch.zeros((batch_size, 2, num_time_points))\n",
        "#     for b in range(batch_size):\n",
        "#       pass # ...\n",
        "#     yield x, y\n",
        "\n",
        "# # Plot a sample of data\n",
        "\n",
        "# x, y = next(batched_data(...)) # this just gets the first item of an iterable\n",
        "\n",
        "# plt.figure(figsize=(12, 5))\n",
        "# for b in range(4):\n",
        "#   # Plot spikes for this batch index\n",
        "#   ax = plt.subplot(2, 4, b+1)\n",
        "#   # ...\n",
        "#   plt.ylabel('Neuron index')\n",
        "#   plt.title(f'Batch index {b}')\n",
        "#   # Plot velocities for this batch index\n",
        "#   plt.subplot(2, 4, b+5, sharex=ax)\n",
        "#   plt.xlabel('Time index')\n",
        "#   plt.ylabel('velocity')\n",
        "#   if b==0:\n",
        "#     plt.legend(loc='best')\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Spiking neural network model\n",
        "\n",
        "In this task, you will write code to simulate a spiking neural network that can be trained using surrogate gradient descent, as in lectures. The neuron model will have some extra features beyond what you have seen in lectures and in the SPyTorch tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU22zpXQT0pq"
      },
      "source": [
        "### Surrogate gradient descent spike function\n",
        "\n",
        "Below is the code for the surrogate gradient descent function from lectures. You can use it as is, although note that there is a hyperparameter (scale) that you can experiment with if you choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJpnQEElTz4i"
      },
      "outputs": [],
      "source": [
        "class SurrogateHeaviside(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements\n",
        "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
        "    we will be able to use all of PyTorch's autograd functionality.\n",
        "    Here we use the normalized negative part of a fast sigmoid\n",
        "    as this was done in Zenke & Ganguli (2018).\n",
        "    \"\"\"\n",
        "\n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it. ctx is a context object that we use to stash information which\n",
        "        we need to later backpropagate our error signals. To achieve this we use the\n",
        "        ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the\n",
        "        surrogate gradient of the loss with respect to the input.\n",
        "        Here we use the normalized negative part of a fast sigmoid\n",
        "        as this was done in Zenke & Ganguli (2018).\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = grad_input/(SurrogateHeaviside.scale*torch.abs(input)+1.0)**2\n",
        "        return grad\n",
        "\n",
        "# here we overwrite our naive spike function by the \"SurrogateHeaviside\" nonlinearity which implements a surrogate gradient\n",
        "surrogate_heaviside  = SurrogateHeaviside.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McTDI4llQjQa"
      },
      "source": [
        "### Task 3A: Single layer simulation code\n",
        "\n",
        "1. ⚠️ Write modular code to simulate a layer of leaky integrate-and-fire spiking neurons compatible with autodifferentiation with PyTorch. The new feature compared to what you have seen before is that the time constant parameter $\\tau$ should be different for each neuron, and be trainable.\n",
        "\n",
        "You can write your simulator as a function or class. My recommendation is to derive from the ``nn.Module`` class of PyTorch, but you can do it how you like. The code should accept an input batch of spikes ``x`` of shape ``(batch_size, num_input_neurons, num_time_points)`` and values 0 or 1 (as in the ``batched_data`` generator function above). The code should have the option to produce either spiking or non-spiking output. In both cases, the output should be an array ``y`` of shape ``(batch_size, num_output_neurons, num_time_points)``. In the case of spiking output, the values of ``y`` should be 0s and 1s, and in the case of non-spiking output they should be the membrane potential values. You may also want to write an additional class to handle multiple layers of spiking neural networks for subsequent tasks.\n",
        "\n",
        "Your code should include initialisation of the weight matrices and time constants, and add additional hyperparameters for this initialisation. I used a spread of time constants from 20-100 ms for spiking neurons and from 200 to 1000 ms for non-spiking neurons and it worked OK, but I didn't do an extensive hyperparameter search and you can probably improve on this.\n",
        "\n",
        "<!-- I would recommend approaching this and the following sections as follows:\n",
        "\n",
        "1. Write simulation code for a single layer, non-spiking neural network first. This code is simpler and will train fast (under 3 minutes on Colab). Attempt as much of the remaining tasks as possible using only this.\n",
        "2. Add the ability for spiking and test that your code produces reasonable output but don't try to train it yet.\n",
        "3. Add the ability to plot spiking hidden layers and try to get a reasonable initialisation of the network.\n",
        "4. Start training the spiking neural network. Your final run will probably take a long time to train but you should build up to that by seeing how well the training curves improve for fewer epochs, batch sizes, etc. -->\n",
        "\n",
        "You can use the template below if you want to use the class-based approach.\n",
        "\n",
        "You may also find it useful to write a class that chains together a group of layers for later parts of the courseowrk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class SNNLayer(nn.Module):\n",
        "#   def __init__(self, n_in, n_out, spiking=True):\n",
        "#     super(SNNLayer, self).__init__()\n",
        "#     self.n_in = n_in\n",
        "#     self.n_out = n_out\n",
        "#     self.spiking = spiking\n",
        "#     # Store weights as a trainable parameter\n",
        "#     self.w = nn.Parameter(torch.ones((n_in, n_out)))\n",
        "#     # ...\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     batch_size, num_neurons, num_time_points = x.shape\n",
        "#     # ...\n",
        "#     return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3B: Verify your code\n",
        "\n",
        "You will verify your code by simulating a network consisting of:\n",
        "\n",
        "* One (non-simulated) input neuron firing at a constant rate of 50 sp/s for a total duration of 1 second simulated with a time step of ``dt=1*ms``.\n",
        "* Two simulated, spiking neurons which each receive spikes from the input neuron with a weight of 0.5. The two simulated neurons should have time constants of 20 ms and 100 ms.\n",
        "\n",
        "You should:\n",
        "\n",
        "1. ⚠️ Run the simulation, recording the membrane potentials and spike times of the simulated neurons.\n",
        "2. ⚠️ Plot the membrane potentials and spike times of the simulated neurons. Print the spike counts of the neurons. The first neuron should not fire any spikes.\n",
        "3. ⚠️ Write down an analytic solution for the first simulated neuron (that doesn't fire any spikes).\n",
        "4. ⚠️ Plot your analytic solution on the same graph as the simulated one to show that it is approximately equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Evaluating fit to data\n",
        "\n",
        "Now that we have our simulation code, we get back to fitting this model to data.\n",
        "\n",
        "1. ⚠️ Write code that takes a network and testing set as input and returns the mean loss over the testing data. The loss is the mean squared error of the output of the network compared to the target data. You may also find it helpful to compute the null loss, which is the loss you would get if you just output all zeros. After training, you should be able to do better than this!\n",
        "2. ⚠️ Write code that plots some of the internal outputs of the network, for example to show you the spikes produced by hidden layers, calculate their firing rates, etc.\n",
        "3. ⚠️ Initialise a network with one hidden layer of 100 spiking neurons and one output layer of 2 non-spiking neurons. Run this on a random sample of the data of length 1 and plot the input spikes, hidden layer spikes, output x- and y-velocities, and the data x- and y-velocities. For each spiking layer compute the firing rates of each neuron and plot them as a histogram.\n",
        "\n",
        "In the next task, you can use this to initialise your networks in a reasonable state. Hidden layers should fire spikes at rates that are not too low and not too high. I aimed for an average firing rate in the range 20-100 and it worked well, but you can experiment with other options. The output layer should give values that are roughly in the right range as the data (i.e. it shouldn't go to +/- 100 if the data is going to only +/- 4). If you look at the spike trains of your hidden layer and all the neurons at initialisation are doing exactly the same thing then it's probably not going to learn very well, so try out some different weight initialisations to see if you can do better.\n",
        "\n",
        "Print the value of the loss (and optionally the null loss) for your untrained network, to give you an idea of the baseline.\n",
        "\n",
        "You may want to wrap your evaluation code in a ``with`` statement like below to stop PyTorch from computing gradients when evaluating (unnecessary and expensive):\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    ...\n",
        "    # whatever you do here won't compute any gradients\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Training\n",
        "\n",
        "At last, we get to the stage of training our network to fit the data.\n",
        "\n",
        "Start with a network consisting of a single non-spiking output layer (i.e. spikes connected directly via a weight matrix to two non-spiking LIF neurons as output).\n",
        "\n",
        "1. ⚠️ Find a good initialisation for this network that gives outputs roughly in the right range.\n",
        "2. ⚠️ Write code to train your network to minimise the mean squared error between the output of your network and the recorded velocities. You will need to select an optimisation algorithm, learning rate, etc.\n",
        "3. ⚠️ Train your network, plotting the loss curves for the training and validation data during training, and print out the test loss at the end.\n",
        "4. ⚠️ Plot the output of your model and compare to the data for 8 randomly sampled time windows of length 1.\n",
        "\n",
        "Don't worry too much about performance at the moment. You should be able to do better than the null loss, but you don't need to do hugely better. I get a null loss of around 1.04 and a test loss of around 0.55, but it's OK if you don't get such good results, and you may get much better results because I didn't do any hyperparameter searching.\n",
        "\n",
        "You may want to use the following code as a starting point (it worked well enough for me but you can probably do better). On my desktop with CPU only, this took about two minutes to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Training parameters\n",
        "# lr = 0.001 # learning rate\n",
        "# num_epochs = 10\n",
        "# max_num_batches = 40\n",
        "# length = 1\n",
        "# batch_size = 32 # small batches worked better for me for some reason\n",
        "\n",
        "# # Optimiser and loss function\n",
        "# optimizer = torch.optim.Adam(..., lr=lr) # what should the first argument be?\n",
        "\n",
        "# # Training\n",
        "# loss_hist = []\n",
        "# val_loss_hist = []\n",
        "# with tqdm(total=num_epochs*max_num_batches) as pbar:\n",
        "#   last_epoch_loss = val_loss = null_val_loss = None\n",
        "#   for epoch in range(num_epochs):\n",
        "#     local_loss = []\n",
        "#     for x, y in batched_data(...):\n",
        "#       # Run the network\n",
        "#       y_out = net(x)\n",
        "#       # Compute a loss\n",
        "#       loss = mse(y_out, y)\n",
        "#       local_loss.append(loss.item())\n",
        "#       # Update gradients\n",
        "#       optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "#       # do something to ensure that taus remain positive, maybe set 1 ms as minimum value\n",
        "#       pbar.update(1)\n",
        "#       pbar.set_postfix(epoch=epoch, last_epoch_loss=last_epoch_loss, loss=loss.item(), val_loss=val_loss, null_val_loss=null_val_loss)\n",
        "#     last_epoch_loss = np.mean(local_loss)\n",
        "#     val_loss, null_val_loss = evaluate_network(net, ...)\n",
        "#     pbar.set_postfix(epoch=epoch, last_epoch_loss=last_epoch_loss, loss=loss.item(), val_loss=val_loss, null_val_loss=null_val_loss)\n",
        "#     loss_hist.append(last_epoch_loss)\n",
        "#     val_loss_hist.append(val_loss)\n",
        "\n",
        "# # Plot the loss function over time\n",
        "# plt.semilogy(loss_hist, label='Testing loss')\n",
        "# plt.semilogy(val_loss_hist, label='Validation loss')\n",
        "# plt.axhline(null_val_loss, ls='--', c='r', label='Null model loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('MSE')\n",
        "# plt.legend(loc='best')\n",
        "# plt.tight_layout()\n",
        "\n",
        "# testing_loss, null_testing_loss = evaluate_network(net, testing_range, length=length, batch_size=batch_size)\n",
        "# print(f'{testing_loss=}, {null_testing_loss=}')\n",
        "\n",
        "# # Plot trained output\n",
        "# plt.figure(figsize=(16, 6))\n",
        "# with torch.no_grad():\n",
        "#   for x, y in batched_data(..., batch_size=8, max_num_batches=1):\n",
        "#     for b in range(8):\n",
        "#       plt.subplot(2, 4, b+1)\n",
        "#       y_out = net(x)\n",
        "#       plt.plot(y_out[b, 0, :], ':C0', label='x_out')\n",
        "#       plt.plot(y_out[b, 1, :], ':C1', label='y_out')\n",
        "#       plt.plot(y[b, 0, :], '--C0', label='x')\n",
        "#       plt.plot(y[b, 1, :], '--C1', label='y')\n",
        "#       # Plot a smoothed version as well\n",
        "#       plt.plot(savgol_filter(y_out[b, 0, :], 151, 3), '-C0', label='x_out (smooth)')\n",
        "#       plt.plot(savgol_filter(y_out[b, 1, :], 151, 3), '-C1', label='y_out (smooth)')\n",
        "#       plt.ylim(-5, 5)\n",
        "# plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 6: Longer length decoding\n",
        "\n",
        "Your code above probably doesn't look great when plotted. That's partly because the outputs start at 0 but the data doesn't necessarily have to, so it takes a while for them to get in sync, and partly because on some intervals it will just do badly. To fix this, and to extend the fit to a longer range of data, in this task we only use the final timestep of each segment and compare to the data. Take a 15 second segment of testing data, and sample every 0.2 seconds to get 75 data points. For each data point, take a 1 second segment of time before this data point (these will be overlapping), run your simulation for that one second, and use the final time point of the simulated output as your prediction. Plot this compared to the real data for 8 different segments of 15 seconds.\n",
        "\n",
        "This should look like a reasonable fit to the data. Congratulations, you have used raw spiking output of neurons recorded from a monkey's brain to predict what it was doing on a computer screen it was interacting with. That's a brain machine interface right here.\n",
        "\n",
        "1. ⚠️ Implement the decoding plot and run it on your trained network.\n",
        "\n",
        "You can use the template below to get you started.\n",
        "\n",
        "Your results might look something like this:\n",
        "\n",
        "![Fits](fits.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def decoding_plot(net, dt_decoding=0.2, decoding_start=1000, decoding_length=15, length=1, dt=1e-3, figdims=(4, 2)):\n",
        "#     w_intervals = np.arange(decoding_start+length, decoding_start+length+decoding_length, dt_decoding)\n",
        "#     batch_size_single = len(w_intervals)-1\n",
        "#     num_time_points = int(np.round(length/dt))\n",
        "#     nfx, nfy = figdims\n",
        "#     nf = nfx*nfy\n",
        "#     batch_size = nf*batch_size_single\n",
        "#     nrows = nfy*2\n",
        "#     ncols = nfx\n",
        "#     with torch.no_grad():\n",
        "#         x = torch.zeros((batch_size, num_neurons, num_time_points))\n",
        "#         y = torch.zeros((batch_size, 2, num_time_points))\n",
        "#         T = []\n",
        "#         for b in range(batch_size):\n",
        "#            w_start = decoding_start+dt_decoding*b\n",
        "#            w_end = w_start+length\n",
        "#            T.append(w_end)\n",
        "#            # ... (copy data to x, y)\n",
        "#         T = np.array(T)\n",
        "#         y_out = ...\n",
        "#         mean_mse = mse(y, y_out)\n",
        "#         plt.figure(figsize=(ncols*3, nrows*2))\n",
        "#         for nf_i in range(nf):\n",
        "#             sp_x = nf_i % nfx\n",
        "#             sp_y = nf_i // nfx\n",
        "#             for i in range(2):\n",
        "#                 plt.subplot(nrows, ncols, sp_x+1+2*ncols*sp_y+i*ncols)\n",
        "#                 # ...\n",
        "#                 plt.ylim(-4, 4)\n",
        "#                 if sp_x==0:\n",
        "#                     plt.ylabel('Velocity')\n",
        "#                 if 2*sp_y+i==nrows-1:\n",
        "#                     plt.xlabel('Time (s)')\n",
        "#                 if nf_i==0:\n",
        "#                     plt.legend(loc='best')\n",
        "#         plt.suptitle(f'{net.fname}: {mean_mse=:.3f}')\n",
        "#         plt.tight_layout()\n",
        "\n",
        "# decoding_plot(models['single_layer.pt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 7: Comparing spiking and non-spiking\n",
        "\n",
        "1. ⚠️ Now try training your network with at least one spiking hidden layer. Compare your results to the non-spiking version. Note that training times with a spiking hidden layer are likely to be much longer. My training time went up from 2 minutes to 30 minutes (with CPU only).\n",
        "2. ⚠️ Plot the trained distribution of weights and time constants for both models.\n",
        "\n",
        "You might find it interesting to compare the distribution of time constants you find for the spiking hidden layer with the results in our paper [Perez et al. 2021](https://neural-reckoning.org/pub_heterogeneity.html). Note: this is not required for credit, only for your interest.\n",
        "\n",
        "Well done - you have made it to the end!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
